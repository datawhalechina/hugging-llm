{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e29809a4",
   "metadata": {},
   "source": [
    "# ChatGPT使用指南——文本生成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbcffab",
   "metadata": {},
   "source": [
    "## 1 引言"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a67093",
   "metadata": {},
   "source": [
    "&emsp;&emsp;自然语言生成任务（Natural Language Generation）是自然语言处理领域内的一个重要研究方向，指的是计算机通过模型或算法，从文本、语音等各种形式的输入中生成自然语言文本输出的过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf31a50",
   "metadata": {},
   "source": [
    "&emsp;&emsp;我们知道，任何知识都能被描述为文本或者说语言，从而先人的智慧才被记录在书卷上代代相传。因此，绝大多数的自然语言处理任务都可以描述为自然语言生成任务，甚至是文本生成任务，将文本作为输入并将新的文本作为输出，这也是近两年大火的T5模型（Text-to-Text Transfer Transformer）的初衷。举例来说，文本分类任务可以理解为输出类别名，如猫/狗、是/否；文本纠错任务可以理解为输入有错误的文本并理解，输出正确的文本描述；智能问答可以理解为根据背景知识及问句进行推理，输出相应的回答……"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a020ab",
   "metadata": {},
   "source": [
    "&emsp;&emsp;可以说，文本生成类任务的应用相当之广，本篇将介绍一些常见的文本生成任务，其中也包含一些曾经并不属于文本生成类任务，但如今也能使用NLG技术进行解决的任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f87fc5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 安装一些必要的包\n",
    "# !pip install openai\n",
    "# # torch install 命令 https://pytorch.org/get-started/locally/\n",
    "# !pip install torch==2.0.0+cpu torchvision==0.15.1+cpu --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "# !pip install tokenizers==0.13.2\n",
    "# !pip install transformers==4.27.4\n",
    "# !pip install --no-binary=protobuf protobuf==3.20.1\n",
    "# !pip install sentencepiece==0.1.97\n",
    "# !pip install shap==2.8.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c80faf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置openai api key\n",
    "import openai\n",
    "OPENAI_API_KEY = \"填入专属的API key\"  # TODO\n",
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f0784bf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['babbage', 'davinci', 'text-davinci-edit-001', 'babbage-code-search-code', 'text-similarity-babbage-001', 'code-davinci-edit-001', 'text-davinci-001', 'ada', 'text-davinci-003', 'babbage-code-search-text', 'babbage-similarity', 'code-search-babbage-text-001', 'text-curie-001', 'whisper-1', 'code-search-babbage-code-001', 'text-ada-001', 'text-embedding-ada-002', 'text-similarity-ada-001', 'curie-instruct-beta', 'ada-code-search-code', 'ada-similarity', 'code-search-ada-text-001', 'text-search-ada-query-001', 'davinci-search-document', 'ada-code-search-text', 'text-search-ada-doc-001', 'davinci-instruct-beta', 'text-similarity-curie-001', 'code-search-ada-code-001', 'ada-search-query', 'text-search-davinci-query-001', 'curie-search-query', 'davinci-search-query', 'babbage-search-document', 'ada-search-document', 'text-search-curie-query-001', 'text-search-babbage-doc-001', 'curie-search-document', 'text-search-curie-doc-001', 'babbage-search-query', 'text-babbage-001', 'text-search-davinci-doc-001', 'text-search-babbage-query-001', 'curie-similarity', 'curie', 'text-similarity-davinci-001', 'text-davinci-002', 'gpt-3.5-turbo-0301', 'gpt-3.5-turbo', 'davinci-similarity', 'cushman:2020-05-03', 'ada:2020-05-03', 'babbage:2020-05-03', 'curie:2020-05-03', 'davinci:2020-05-03', 'if-davinci-v2', 'if-curie-v2', 'if-davinci:3.0.0', 'davinci-if:3.0.0', 'davinci-instruct-beta:2.0.0', 'text-ada:001', 'text-davinci:001', 'text-curie:001', 'text-babbage:001', 'ada:ft-personal-2023-04-15-11-57-25', 'ada:ft-personal-2023-04-15-12-54-03', 'ada:ft-personal-2023-04-15-13-19-25', 'ada:ft-personal-2023-04-15-13-29-50']\n"
     ]
    }
   ],
   "source": [
    "models = openai.Model.list()\n",
    "print([x.id for x in models.data])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627fdd54",
   "metadata": {},
   "source": [
    "&emsp;&emsp;模型列表中包含内置的60多个可用模型，以及自己fine tune的模型，fine tune模型以\"ft-personal\"开头。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2aa891",
   "metadata": {},
   "source": [
    "## 2 文本摘要任务"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f08e03",
   "metadata": {},
   "source": [
    "### 2.1 什么是文本摘要？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92c1753",
   "metadata": {},
   "source": [
    "&emsp;&emsp;文本摘要任务指的是用精炼的文本来概括整篇文章的大意，使得用户能够通过阅读摘要来大致了解文章的主要内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c92302",
   "metadata": {},
   "source": [
    "### 2.2 常见的文本摘要技术"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150f7f57",
   "metadata": {},
   "source": [
    "&emsp;&emsp;从实现手法来说，文本摘要任务主要分为以下三种：\n",
    "\n",
    "- 抽取式摘要：从原文档中提取现成的句子作为摘要句。\n",
    "- 压缩式摘要：对原文档的冗余信息进行过滤，压缩文本作为摘要。\n",
    "- 生成式摘要：基于NLG技术，根据源文档内容，由算法模型自己生成自然语言描述。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ee6568",
   "metadata": {},
   "source": [
    "&emsp;&emsp;以下是一个基于mT5模型（T5模型的多语言版）的文本摘要样例。\n",
    "\n",
    "**注：**下载模型较大，可前往huggingface->Hosted inference API 在线测试。https://huggingface.co/csebuetnlp/mT5_multilingual_XLSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fea41313",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始文本:  自动信任协商主要解决跨安全域的信任建立问题,使陌生实体通过反复的、双向的访问控制策略和数字证书的相互披露而逐步建立信任关系。由于信任建立的方式独特和应用环境复杂,自动信任协商面临多方面的安全威胁,针对协商的攻击大多超出常规防范措施所保护的范围,因此有必要对自动信任协商中的攻击手段进行专门分析。按攻击特点对自动信任协商中存在的各种攻击方式进行分类,并介绍了相应的防御措施,总结了当前研究工作的不足,对未来的研究进行了展望\n",
      "摘要文本:  自动信任协商(AI)是互信关系建立的最新研究工作的一部分。\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    " \n",
    "# 载入模型 \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"csebuetnlp/mT5_multilingual_XLSum\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"csebuetnlp/mT5_multilingual_XLSum\")\n",
    "\n",
    "WHITESPACE_HANDLER = lambda k: re.sub('\\s+', ' ', re.sub('\\n+', ' ', k.strip()))\n",
    "\n",
    "text = \"\"\"自动信任协商主要解决跨安全域的信任建立问题,使陌生实体通过反复的、双向的访问控制策略和数字证书的相互披露而逐步建立信任关系。由于信任建立的方式独特和应用环境复杂,自动信任协商面临多方面的安全威胁,针对协商的攻击大多超出常规防范措施所保护的范围,因此有必要对自动信任协商中的攻击手段进行专门分析。按攻击特点对自动信任协商中存在的各种攻击方式进行分类,并介绍了相应的防御措施,总结了当前研究工作的不足,对未来的研究进行了展望\"\"\"\n",
    "text = WHITESPACE_HANDLER(text)\n",
    "input_ids = tokenizer([text], return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)[\"input_ids\"]\n",
    "\n",
    "# 生成结果文本\n",
    "output_ids = model.generate(input_ids=input_ids, max_length=84, no_repeat_ngram_size=2, num_beams=4)[0]\n",
    "output_text = tokenizer.decode(output_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "\n",
    "print(\"原始文本: \", text)\n",
    "print(\"摘要文本: \", output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5336515f",
   "metadata": {},
   "source": [
    "### 2.3 基于OpenAI接口的文本摘要实验"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93927dd2",
   "metadata": {},
   "source": [
    "#### 2.3.1 简单上手版：调用预训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02cae11",
   "metadata": {},
   "source": [
    "**GPT 3.5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbcc7b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始文本:  自动信任协商主要解决跨安全域的信任建立问题,使陌生实体通过反复的、双向的访问控制策略和数字证书的相互披露而逐步建立信任关系。由于信任建立的方式独特和应用环境复杂,自动信任协商面临多方面的安全威胁,针对协商的攻击大多超出常规防范措施所保护的范围,因此有必要对自动信任协商中的攻击手段进行专门分析。按攻击特点对自动信任协商中存在的各种攻击方式进行分类,并介绍了相应的防御措施,总结了当前研究工作的不足,对未来的研究进行了展望。\n",
      "摘要文本:  自动信任协商解决跨域信任建立,存在多种安全威胁,提出了防御措施和研究展望。\n",
      "摘要文本长度:  37\n"
     ]
    }
   ],
   "source": [
    "def summarize_text(text):\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=f\"请对以下文本进行总结，注意总结的凝炼性，将总结字数控制在20个字以内:\\n{text}\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=500,\n",
    "    )\n",
    "\n",
    "    summarized_text = response.choices[0].text.strip()\n",
    "    return summarized_text\n",
    "\n",
    "text = \"自动信任协商主要解决跨安全域的信任建立问题,使陌生实体通过反复的、双向的访问控制策略和数字证书的相互披露而逐步建立信任关系。由于信任建立的方式独特和应用环境复杂,自动信任协商面临多方面的安全威胁,针对协商的攻击大多超出常规防范措施所保护的范围,因此有必要对自动信任协商中的攻击手段进行专门分析。按攻击特点对自动信任协商中存在的各种攻击方式进行分类,并介绍了相应的防御措施,总结了当前研究工作的不足,对未来的研究进行了展望。\"\"\"\n",
    "output_text = summarize_text(text)\n",
    "print(\"原始文本: \", text)\n",
    "print(\"摘要文本: \", output_text)\n",
    "print(\"摘要文本长度: \", len(output_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fee2ff",
   "metadata": {},
   "source": [
    "**ChatGPT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "296ad3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始文本:  自动信任协商主要解决跨安全域的信任建立问题,使陌生实体通过反复的、双向的访问控制策略和数字证书的相互披露而逐步建立信任关系。由于信任建立的方式独特和应用环境复杂,自动信任协商面临多方面的安全威胁,针对协商的攻击大多超出常规防范措施所保护的范围,因此有必要对自动信任协商中的攻击手段进行专门分析。按攻击特点对自动信任协商中存在的各种攻击方式进行分类,并介绍了相应的防御措施,总结了当前研究工作的不足,对未来的研究进行了展望。\n",
      "摘要文本:  总结：自动信任协商解决跨安全域的信任建立问题，但面临多方面安全威胁，需加强攻击防护与研究。\n",
      "摘要文本长度:  45\n"
     ]
    }
   ],
   "source": [
    "def summarize_text(text):\n",
    "    content = f\"请对以下文本进行总结，注意总结的凝炼性，将总结字数控制在20个字以内:\\n{text}\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\", \n",
    "        messages=[{\"role\": \"user\", \"content\": content}]\n",
    "    )\n",
    "    summarized_text = response.get(\"choices\")[0].get(\"message\").get(\"content\")\n",
    "    return summarized_text\n",
    "\n",
    "text = \"\"\"自动信任协商主要解决跨安全域的信任建立问题,使陌生实体通过反复的、双向的访问控制策略和数字证书的相互披露而逐步建立信任关系。由于信任建立的方式独特和应用环境复杂,自动信任协商面临多方面的安全威胁,针对协商的攻击大多超出常规防范措施所保护的范围,因此有必要对自动信任协商中的攻击手段进行专门分析。按攻击特点对自动信任协商中存在的各种攻击方式进行分类,并介绍了相应的防御措施,总结了当前研究工作的不足,对未来的研究进行了展望。\"\"\"\n",
    "output_text = summarize_text(text)\n",
    "\n",
    "print(\"原始文本: \", text)\n",
    "print(\"摘要文本: \", output_text)\n",
    "print(\"摘要文本长度: \", len(output_text))\n",
    "# 注意，chatgpt并不能完美限制摘要输出的字数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db60c921",
   "metadata": {},
   "source": [
    "#### 2.3.2 进阶优化版：基于自定义语料fine tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13685276",
   "metadata": {},
   "source": [
    "数据集来源：CSL摘要数据集，是计算机领域的论文摘要和标题数据，包含3500条数据，\n",
    "\n",
    "- 标题：平均字数 18，字数标准差 4，最大字数41，最小数字 6；\n",
    "- 正文：平均字数 200，字数标准差 63，最大字数 631，最小数字 41；\n",
    "\n",
    "数据源地址：https://github.com/liucongg/GPT2-NewsTitle 项目中的CSL摘要数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "550befb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('dataset/csl_data.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe352a16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': '自动信任协商中的攻击与防范',\n",
       " 'content': '自动信任协商主要解决跨安全域的信任建立问题,使陌生实体通过反复的、双向的访问控制策略和数字证书的相互披露而逐步建立信任关系。由于信任建立的方式独特和应用环境复杂,自动信任协商面临多方面的安全威胁,针对协商的攻击大多超出常规防范措施所保护的范围,因此有必要对自动信任协商中的攻击手段进行专门分析。按攻击特点对自动信任协商中存在的各种攻击方式进行分类,并介绍了相应的防御措施,总结了当前研究工作的不足,对未来的研究进行了展望。'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ede7d816",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>提出了一种新的保细节的变形算法,可以使网格模型进行尽量刚性的变形,以减少变形中几何细节的扭曲...</td>\n",
       "      <td>保细节的网格刚性变形算法</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>实时服装动画生成技术能够为三维虚拟角色实时地生成逼真的服装动态效果,在游戏娱乐、虚拟服装设计...</td>\n",
       "      <td>一种基于混合模型的实时虚拟人服装动画方法</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>提出一种基于模糊主分量分析技术(FPCA)的人脸遮挡检测与去除方法.首先,有遮挡人脸被投影到...</td>\n",
       "      <td>人脸遮挡区域检测与重建</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>图像匹配技术在计算机视觉、遥感和医学图像分析等领域有着广泛的应用背景.针对传统的相关匹配算法...</td>\n",
       "      <td>一种基于奇异值分解的图像匹配算法</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>提出了一种基于片相似性的各项异性扩散图像去噪方法.传统的各项异性图像去噪方法都是基于单个像素...</td>\n",
       "      <td>片相似性各项异性扩散图像去噪</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt            completion\n",
       "0  提出了一种新的保细节的变形算法,可以使网格模型进行尽量刚性的变形,以减少变形中几何细节的扭曲...          保细节的网格刚性变形算法\n",
       "1  实时服装动画生成技术能够为三维虚拟角色实时地生成逼真的服装动态效果,在游戏娱乐、虚拟服装设计...  一种基于混合模型的实时虚拟人服装动画方法\n",
       "2  提出一种基于模糊主分量分析技术(FPCA)的人脸遮挡检测与去除方法.首先,有遮挡人脸被投影到...           人脸遮挡区域检测与重建\n",
       "3  图像匹配技术在计算机视觉、遥感和医学图像分析等领域有着广泛的应用背景.针对传统的相关匹配算法...      一种基于奇异值分解的图像匹配算法\n",
       "4  提出了一种基于片相似性的各项异性扩散图像去噪方法.传统的各项异性图像去噪方法都是基于单个像素...        片相似性各项异性扩散图像去噪"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data)\n",
    "df = df[['content', 'title']]\n",
    "df.columns = [\"prompt\", \"completion\"]\n",
    "df_train = df.iloc[:500]\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb41caa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_json(\"dataset/csl_summarize_finetune.jsonl\", orient='records', lines=True, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01eac86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing...\n",
      "\n",
      "- Your file contains 500 prompt-completion pairs\n",
      "- More than a third of your `prompt` column/key is uppercase. Uppercase prompts tends to perform worse than a mixture of case encountered in normal language. We recommend to lower case the data if that makes sense in your domain. See https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more details\n",
      "- More than a third of your `completion` column/key is uppercase. Uppercase completions tends to perform worse than a mixture of case encountered in normal language. We recommend to lower case the data if that makes sense in your domain. See https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more details\n",
      "- Your data does not contain a common separator at the end of your prompts. Having a separator string appended to the end of the prompt makes it clearer to the fine-tuned model where the completion should begin. See https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more detail and examples. If you intend to do open-ended generation, then you should leave the prompts empty\n",
      "- Your data does not contain a common ending at the end of your completions. Having a common ending string appended to the end of the completion makes it clearer to the fine-tuned model where the completion should end. See https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more detail and examples.\n",
      "- The completion should start with a whitespace character (` `). This tends to produce better results due to the tokenization we use. See https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more details\n",
      "\n",
      "Based on the analysis we will perform the following actions:\n",
      "- [Recommended] Lowercase all your data in column/key `prompt` [Y/n]: Y\n",
      "- [Recommended] Lowercase all your data in column/key `completion` [Y/n]: Y\n",
      "- [Recommended] Add a suffix separator ` ->` to all prompts [Y/n]: Y\n",
      "- [Recommended] Add a suffix ending `\\n` to all completions [Y/n]: Y\n",
      "- [Recommended] Add a whitespace character to the beginning of the completion [Y/n]: Y\n",
      "\n",
      "\n",
      "Your data will be written to a new JSONL file. Proceed [Y/n]: Y\n",
      "\n",
      "Wrote modified file to `dataset/csl_summarize_finetune_prepared.jsonl`\n",
      "Feel free to take a look!\n",
      "\n",
      "Now use that file when fine-tuning:\n",
      "> openai api fine_tunes.create -t \"dataset/csl_summarize_finetune_prepared.jsonl\"\n",
      "\n",
      "After you’ve fine-tuned a model, remember that your prompt has to end with the indicator string ` ->` for the model to start generating completions, rather than continuing with the prompt. Make sure to include `stop=[\"\\n\"]` so that the generated texts ends at the expected place.\n",
      "Once your model starts training, it'll approximately take 9.31 minutes to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.\n"
     ]
    }
   ],
   "source": [
    "!openai tools fine_tunes.prepare_data -f dataset/csl_summarize_finetune.jsonl -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8e3ef6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ.setdefault(\"OPENAI_API_KEY\", OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef051974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from ./dataset/csl_summarize_finetune_prepared.jsonl: file-S3SIEZoJbqPXTGT16YxPThVO\n",
      "Created fine-tune: ft-LoKi6mOxlkOtfZcZTrmivKDa\n",
      "Streaming events until fine-tuning is complete...\n",
      "\n",
      "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
      "[2023-04-15 21:23:56] Created fine-tune: ft-LoKi6mOxlkOtfZcZTrmivKDa\n",
      "[2023-04-15 21:24:05] Fine-tune costs $0.43\n",
      "[2023-04-15 21:24:05] Fine-tune enqueued. Queue number: 0\n",
      "[2023-04-15 21:24:06] Fine-tune started\n",
      "\n",
      "Stream interrupted (client disconnected).\n",
      "To resume the stream, run:\n",
      "\n",
      "  openai api fine_tunes.follow -i ft-LoKi6mOxlkOtfZcZTrmivKDa\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Upload progress:   0%|          | 0.00/380k [00:00<?, ?it/s]\n",
      "Upload progress: 100%|██████████| 380k/380k [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "!openai api fine_tunes.create \\\n",
    "    -t \"./dataset/csl_summarize_finetune_prepared.jsonl\" \\\n",
    "    -m ada\\\n",
    "    --no_check_if_files_exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbcbc46b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"created_at\": 1681565036,\n",
      "  \"events\": [\n",
      "    {\n",
      "      \"created_at\": 1681565036,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Created fine-tune: ft-LoKi6mOxlkOtfZcZTrmivKDa\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1681565045,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Fine-tune costs $0.43\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1681565045,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Fine-tune enqueued. Queue number: 0\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1681565046,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Fine-tune started\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1681565139,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Completed epoch 1/4\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1681565216,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Completed epoch 2/4\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1681565293,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Completed epoch 3/4\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1681565369,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Completed epoch 4/4\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1681565391,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Uploaded model: ada:ft-personal-2023-04-15-13-29-50\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1681565392,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Uploaded result file: file-TTRfuuyBXuZ4BKwX9I4i2zA4\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1681565392,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Fine-tune succeeded\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    }\n",
      "  ],\n",
      "  \"fine_tuned_model\": \"ada:ft-personal-2023-04-15-13-29-50\",\n",
      "  \"hyperparams\": {\n",
      "    \"batch_size\": 1,\n",
      "    \"learning_rate_multiplier\": 0.1,\n",
      "    \"n_epochs\": 4,\n",
      "    \"prompt_loss_weight\": 0.01\n",
      "  },\n",
      "  \"id\": \"ft-LoKi6mOxlkOtfZcZTrmivKDa\",\n",
      "  \"model\": \"ada\",\n",
      "  \"object\": \"fine-tune\",\n",
      "  \"organization_id\": \"org-U35hu1wdD7w3HnkgJ5fdBW8m\",\n",
      "  \"result_files\": [\n",
      "    {\n",
      "      \"bytes\": 112280,\n",
      "      \"created_at\": 1681565392,\n",
      "      \"filename\": \"compiled_results.csv\",\n",
      "      \"id\": \"file-TTRfuuyBXuZ4BKwX9I4i2zA4\",\n",
      "      \"object\": \"file\",\n",
      "      \"purpose\": \"fine-tune-results\",\n",
      "      \"status\": \"processed\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ],\n",
      "  \"status\": \"succeeded\",\n",
      "  \"training_files\": [\n",
      "    {\n",
      "      \"bytes\": 380384,\n",
      "      \"created_at\": 1681565036,\n",
      "      \"filename\": \"./dataset/csl_summarize_finetune_prepared.jsonl\",\n",
      "      \"id\": \"file-S3SIEZoJbqPXTGT16YxPThVO\",\n",
      "      \"object\": \"file\",\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"status\": \"processed\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ],\n",
      "  \"updated_at\": 1681565392,\n",
      "  \"validation_files\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# 根据上一步的输出，得到fine tune运行的key ft-LoKi6mOxlkOtfZcZTrmivKDa，\n",
    "# 我们可以通过get来获取当前执行进度，\n",
    "# 如发现与openai的连接断开，可通过follow重新排队连接\n",
    "# !openai api fine_tunes.follow -i ft-LoKi6mOxlkOtfZcZTrmivKDa\n",
    "!openai api fine_tunes.get -i ft-LoKi6mOxlkOtfZcZTrmivKDa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0ed20b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存openai fine tune过程的记录\n",
    "!openai api fine_tunes.results -i ft-cSvqpGrrohBdPPmE7oxR2Xy3 > dataset/metric.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6611d087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始文本:  自动信任协商主要解决跨安全域的信任建立问题,使陌生实体通过反复的、双向的访问控制策略和数字证书的相互披露而逐步建立信任关系。由于信任建立的方式独特和应用环境复杂,自动信任协商面临多方面的安全威胁,针对协商的攻击大多超出常规防范措施所保护的范围,因此有必要对自动信任协商中的攻击手段进行专门分析。按攻击特点对自动信任协商中存在的各种攻击方式进行分类,并介绍了相应的防御措施,总结了当前研究工作的不足,对未来的研究进行了展望。\n",
      "ada摘要文本:  下面我们以HRC的经验为例,以免影响攻击的防御能力,使国内研究中的高等学术人员更加紧接地接受高等\n",
      "ada fine-tune摘要文本:  -> 自动信任协商的防御措施分类与总结\n",
      "\n",
      "3.2.自动信任协商中的攻击研究\n",
      "\n",
      "研究综述了防御措施分\n"
     ]
    }
   ],
   "source": [
    "def summarize_text(text, model_name):\n",
    "    response = openai.Completion.create(\n",
    "        engine=model_name,\n",
    "        prompt=f\"请对以下文本进行总结，注意总结的凝炼性，将总结字数控制在20个字以内:\\n{text}\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=100,\n",
    "    )\n",
    "\n",
    "    summarized_text = response.choices[0].text.strip()\n",
    "    return summarized_text\n",
    "\n",
    "text = \"自动信任协商主要解决跨安全域的信任建立问题,使陌生实体通过反复的、双向的访问控制策略和数字证书的相互披露而逐步建立信任关系。由于信任建立的方式独特和应用环境复杂,自动信任协商面临多方面的安全威胁,针对协商的攻击大多超出常规防范措施所保护的范围,因此有必要对自动信任协商中的攻击手段进行专门分析。按攻击特点对自动信任协商中存在的各种攻击方式进行分类,并介绍了相应的防御措施,总结了当前研究工作的不足,对未来的研究进行了展望。\"\"\"\n",
    "print(\"原始文本: \", text)\n",
    "print(\"ada摘要文本: \", summarize_text(text, model_name='ada'))\n",
    "print(\"ada fine-tune摘要文本: \", summarize_text(text, model_name='ada:ft-personal-2023-04-15-13-29-50'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd27ef1",
   "metadata": {},
   "source": [
    "&emsp;&emsp;由于资费与效率原因，本次实验基于Ada模型进行fine tune。可以看到，原始的Ada模型几乎完全没有get到文本摘要任务，只是在文本背景上生成了一段新的文本。在经过简单的fine tune后，虽然生成的文本仍然远不及ChatGPT或者其他在该任务上做过精细微调的大模型，但是已经能在一定程度上生成一个还算不错的摘要了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf8f1c6",
   "metadata": {},
   "source": [
    "## 3 文本纠错任务"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d4417e",
   "metadata": {},
   "source": [
    "### 3.1 什么是文本纠错？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bfb7bb",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在日常生活中，不管是微信聊天、微博推文甚至是出版书籍中，我们都或多或少地会发现文本中的错别字现象。\n",
    "\n",
    "&emsp;&emsp;这些错别字可能源于语音输入时的口音偏差，如“飞机”被识别成了“灰机”；也可能是拼音输入时误触了临近键位或者选错了结果，如“飞机”被识别成了“得急”、“肥鸡”；亦或是手写输入时写成了形近字，如“战栗”被识别为了“战粟”……"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d0ced8",
   "metadata": {},
   "source": [
    "&emsp;&emsp;常见的错误类型包括：\n",
    "\n",
    "- 拼写错误：中文课程->中文磕碜；明天会议->明天会易\n",
    "- 语法错误：他昨天去参加会议了->他昨天将要去参加会议\n",
    "- 标点符号错误：您好，请多指教！->您好,请多指教???\n",
    "- 知识性错误：上海黄浦区->上海黄埔区\n",
    "- 重复性错误：您好，请问您今天有空吗？->您好，请问您今天有空吗吗吗吗吗吗\n",
    "- 遗漏性错误：他昨天去参加会议了->他昨天去参加了\n",
    "- 语序性错误：他昨天去参加会议了->他昨天去会议参加了\n",
    "- 多语言错误：他昨天去参加会议了->他昨天去参加huiyi了\n",
    "- ……"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3105849a",
   "metadata": {},
   "source": [
    "&emsp;&emsp;总之，文本错误可能是千奇百怪的。对于人类而言，凭借着常识与上下文，实现语义理解尚不是什么难事，有时只是些许影响阅读体验；而对于一些特定的文本下游任务，如命名实体识别或意图识别，一条不加处理的错误输入文本可能会导致南辕北辙的识别结果。|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6db508",
   "metadata": {},
   "source": [
    "&emsp;&emsp;文本纠错任务指的是通过自然语言处理技术对文本中出现的错误进行检测和纠正的过程。目前已经成为自然语言处理领域中的一个重要分支，被广泛地应用于搜索引擎、机器翻译、智能客服等各种领域。纵然由于文本错误的多样性，我们往往难以将所有错误通通识别并纠正成功，但是如果能尽可能多且正确地识别文本中的错误，能够大大降低人工审核的成本，也不失为一桩美事~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b9c8bd",
   "metadata": {},
   "source": [
    "### 3.2 常见的文本纠错技术"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72227488",
   "metadata": {},
   "source": [
    "&emsp;&emsp;常见的文本纠错技术主要有以下几种：\n",
    "\n",
    "1. 基于规则的文本纠错技术\n",
    "2. 基于语言模型的文本纠错技术\n",
    "3. 基于MLM的文本纠错技术\n",
    "4. 基于NLG的文本纠错技术"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66117c5",
   "metadata": {},
   "source": [
    "#### 3.2.1 基于规则的文本纠错技术"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33826ba",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这种文本纠错技术是通过实现定义的规则来检查文本中的拼写、语法、标点符号等常见错误，假如“金字塔”常被误写为“金子塔”，则在数据库中加入两者的映射关系。由于这种传统方法需要大量的人工工作以及专家对于语言的深刻理解，因此难以处理海量文本或较为复杂的语言错误。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9cc3be",
   "metadata": {},
   "source": [
    "#### 3.2.2 基于语言模型的文本纠错技术"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c884d366",
   "metadata": {},
   "source": [
    "&emsp;&emsp;基于语言模型的文本纠错技术包括错误检测和错误纠正，这种方法同样比较简单粗暴，方法速度快，扩展性强，效果一般。常见的模型有Kenlm。\n",
    "\n",
    "- 错误检测：使用`jieba`中文分词器对句子进行切词，然后结合字粒度和词粒度两方面的疑似错误结果，形成疑似错误位置候选集。\n",
    "\n",
    "- 错误纠正：遍历所有的候选集并使用音似、形似词典替换错误位置的词，然后通过语言模型计算句子困惑度，最后比较并排序所有候选集结果，得到最优纠正词。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ebb3e7",
   "metadata": {},
   "source": [
    "#### 3.2.3 基于MLM的文本纠错技术"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891b1797",
   "metadata": {},
   "source": [
    "&emsp;&emsp;我们知道，BERT在预训练阶段使用了Masked Language Model掩码语言模型（MLM）及Next Sentence Prediction下一句预测（NSP）两个任务，其中MLM任务中有15%\\*10%的Token会被替换为随机的其他词汇，迫使模型更多地依赖于上下文信息去预测Mask词汇，在一定程度上赋予了模型纠错能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f61656",
   "metadata": {},
   "source": [
    "&emsp;&emsp;因此，我们将BERT的MLM任务做一下简单的修改，将输入设计为错误的词汇，输出为正确的词汇，做一下简单的fine tune，即可轻松实现文本纠错功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb92ca7",
   "metadata": {},
   "source": [
    "&emsp;&emsp;例如，ACL2020的Soft-Masked BERT模型（[论文笔记](https://www.cnblogs.com/peng-yuan/p/15412346.html)），设计了一个二重网络来进行文本纠错，其中“错误检测网络”通过Bi-GRU识别每个字符错误的概率，“错误纠正网络”倾向将错误概率更高的词Mask掉，并预测出真实词汇。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cd959f",
   "metadata": {},
   "source": [
    "#### 3.2.4 基于NLG的文本纠错技术"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f79ae4",
   "metadata": {},
   "source": [
    "&emsp;&emsp;上述提到的Mask方法只能用于输入与输出等长的情况，但是实际应用中往往会出现两者不等长的情况，如错字或多字。一种可能的解决办法是，在原有的BERT模型后嵌入一层Transformer Decoder，即将“文本纠错”任务等价于“将错误的文本翻译成正确的文本”，此时我们没法保证输出文本与原始文本中正确的部分一定能保持完全一致，可能在语义不变的情况下，会生成了一种新的表达方式。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e576f084",
   "metadata": {},
   "source": [
    "#### 3.2.5 一个文本纠错工具集：pycorrector "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418c9cac",
   "metadata": {},
   "source": [
    "&emsp;&emsp;pycorrector是一个文本纠错工具集，内置了KenLM、MacBERT、Transformer等多种文本纠错模型。\n",
    "\n",
    "- pycorrector的项目地址：https://github.com/shibing624/pycorrector\n",
    "- 一个基于MacBERT的线上Demo：https://huggingface.co/spaces/shibing624/pycorrector\n",
    "\n",
    "&emsp;&emsp;pycorrector不仅可以通过“import pycorrector”调用，也提供了Huggingface的预训练模型调用方式，以下是一个基于Huggingface的MacBERT4CSC调用样例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80cf2589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始文本:  大家好,一起来参加DataWhale的《ChatGPT使用指南》组队学习课乘吧！\n",
      "纠错文本:  大家好,一起来参加datawhale的《chatgpt使用指南》组队学习课程吧！\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "# 载入模型\n",
    "tokenizer = BertTokenizer.from_pretrained(\"shibing624/macbert4csc-base-chinese\")\n",
    "model = BertForMaskedLM.from_pretrained(\"shibing624/macbert4csc-base-chinese\")\n",
    "\n",
    "text = \"大家好,一起来参加DataWhale的《ChatGPT使用指南》组队学习课乘吧！\"\n",
    "input_ids = tokenizer([text], padding=True, return_tensors='pt')\n",
    "\n",
    "# 生成结果文本\n",
    "with torch.no_grad():\n",
    "    outputs = model(**input_ids)\n",
    "output_ids = torch.argmax(outputs.logits, dim=-1)\n",
    "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True).replace(' ', '')\n",
    "\n",
    "print(\"原始文本: \", text)\n",
    "print(\"纠错文本: \", output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "458324c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('乘', '程', 37, 38)]\n"
     ]
    }
   ],
   "source": [
    "# 查看修改点\n",
    "import operator\n",
    "def get_errors(corrected_text, origin_text):\n",
    "    sub_details = []\n",
    "    for i, ori_char in enumerate(origin_text):\n",
    "        if ori_char in [' ', '“', '”', '‘', '’', '琊', '\\n', '…', '—', '擤']:\n",
    "            # add unk word\n",
    "            corrected_text = corrected_text[:i] + ori_char + corrected_text[i:]\n",
    "            continue\n",
    "        if i >= len(corrected_text):\n",
    "            continue\n",
    "        if ori_char != corrected_text[i]:\n",
    "            if ori_char.lower() == corrected_text[i]:\n",
    "                # pass english upper char\n",
    "                corrected_text = corrected_text[:i] + ori_char + corrected_text[i + 1:]\n",
    "                continue\n",
    "            sub_details.append((ori_char, corrected_text[i], i, i + 1))\n",
    "    sub_details = sorted(sub_details, key=operator.itemgetter(2))\n",
    "    return corrected_text, sub_details\n",
    "\n",
    "correct_text, details = get_errors(output_text[:len(text)], text)\n",
    "print(details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfe3636",
   "metadata": {},
   "source": [
    "## 3.3 基于OpenAI接口的文本纠错实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45676106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始文本:  大家好,一起来参加DataWhale的《ChatGPT使用指南》组队学习课乘吧！\n",
      "纠错文本:  大家好，一起来参加DataWhale的《ChatGPT使用指南》组队学习课程吧！\n"
     ]
    }
   ],
   "source": [
    "def correct_text(text):\n",
    "    content = f\"请对以下文本进行文本纠错:\\n{text}\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\", \n",
    "        messages=[{\"role\": \"user\", \"content\": content}]\n",
    "    )\n",
    "    corrected_text = response.get(\"choices\")[0].get(\"message\").get(\"content\")\n",
    "    return corrected_text\n",
    "\n",
    "text = \"大家好,一起来参加DataWhale的《ChatGPT使用指南》组队学习课乘吧！\"\n",
    "output_text = correct_text(text)\n",
    "print(\"原始文本: \", text)\n",
    "print(\"纠错文本: \", output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9c9605",
   "metadata": {},
   "source": [
    "## 4 机器翻译"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0e26c8",
   "metadata": {},
   "source": [
    "### 4.1 什么是机器翻译？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a161f9a5",
   "metadata": {},
   "source": [
    "&emsp;&emsp;机器翻译，又称为自动翻译，是利用计算机将一种自然语言（源语言）转换为另一种自然语言（目标语言）的过程。据不完全统计，世界上约有7000种语言，两两配对约有 $7000^2$ 种组合，这些语言中又不乏一词多义、垂类知识等现象，因此能够使用更少的标注数据，或者无监督地让计算机真正地理解输入语言的含义，并“信”、“达”、“雅”地转化为输出语言，是历来学者们的研究重心。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab07c67e",
   "metadata": {},
   "source": [
    "&emsp;&emsp;众所周知，机器翻译一直是自然语言处理领域备受关注的研究方向，也是自然语言处理技术最早展露头角的任务之一。如今市面上的机器翻译工具层出不穷，如大家常用的百度翻译、谷歌翻译，乃至小时候科幻片里才有的AI同声传译，如讯飞听见同传。简单来说可以将其划分为通用领域（多语种）、垂直领域、术语定制化、领域自适应、人工适应、语音翻译等。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bded3d",
   "metadata": {},
   "source": [
    "### 4.2 常见的机器翻译技术"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eba62c",
   "metadata": {},
   "source": [
    "&emsp;&emsp;从机器翻译的发展历程来看，主要经历了如下几个阶段：\n",
    "\n",
    "- 基于规则的方法\n",
    "- 基于统计的方法\n",
    "- 基于神经网络的方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd423738",
   "metadata": {},
   "source": [
    "&emsp;&emsp;基于规则的方法需要建立各类知识库，描述源语言和目标语言的词法、句法以及语义知识，有时知识无关的世界知识。\n",
    "\n",
    "&emsp;&emsp;基于统计的方法认为对于一条源语言 $R$，任何一条目标语言 $T$ 都可能是它的译文，只是可能性有高有低。对于源语言中的每个词 $r_i$ 及目标语言中的每个词 $t_j$，判断词对齐的概率，再通过期望最大算法（如EM算法）得到最大词对齐概率的对齐方式。这便是基于词的翻译模型。显然，将翻译的最小单位设计成词是不符合语法的，因此后来又延申出了基于短语的翻译方法，将最小翻译单位设计成连续的词串。\n",
    "\n",
    "&emsp;&emsp;2013年，一种用于机器翻译的新型端到端编码器-解码器架构问世，将CNN用于隐含表征挖掘，将RNN用于将隐含向量转化为目标语言，标志了神经机器翻译开端。后来，Attention、Transformer、BERT等技术被相继提出，大大提升了翻译的质量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b17904",
   "metadata": {},
   "source": [
    "&emsp;&emsp;以下是一个基于transformers实现机器翻译的简单样例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4841c938",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始文本:  大家好，一起来参加DataWhale的《ChatGPT使用指南》组队学习课程吧！\n",
      "翻译文本:  Hey, guys, let's join the ChatGPT team at DataWhale.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-zh-en\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-zh-en\")\n",
    "\n",
    "text = \"大家好，一起来参加DataWhale的《ChatGPT使用指南》组队学习课程吧！\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", )\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=40, num_beams=4, early_stopping=True)\n",
    "translated_sentence = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print('原始文本: ', text)\n",
    "print('翻译文本: ', translated_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9ba0a4",
   "metadata": {},
   "source": [
    "### 4.3 基于OpenAI接口的机器翻译实验"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7f8f21",
   "metadata": {},
   "source": [
    "#### 4.3.1 简单上手版：短文本英翻中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cf1835e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始文本:  大家好，一起来参加DataWhale的《ChatGPT使用指南》组队学习课程吧！\n",
      "输出文本:  Hello everyone, let's join the team learning course of \"ChatGPT User Guide\" organized by DataWhale together!\n"
     ]
    }
   ],
   "source": [
    "def translate_text(text):\n",
    "    content = f\"请将以下中文文本翻译成英文:\\n{text}\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\", \n",
    "        messages=[{\"role\": \"user\", \"content\": content}]\n",
    "    )\n",
    "    translated_text = response.get(\"choices\")[0].get(\"message\").get(\"content\")\n",
    "    return translated_text\n",
    "\n",
    "text_to_translate = \"大家好，一起来参加DataWhale的《ChatGPT使用指南》组队学习课程吧！\"\n",
    "translated_text = translate_text(text_to_translate)\n",
    "print(\"原始文本: \", text_to_translate)\n",
    "print(\"输出文本: \", translated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36943628",
   "metadata": {},
   "source": [
    "&emsp;&emsp;可以看到，ChatGPT明显比Helsinki-NLP在中翻英上的效果更好，将《ChatGPT使用指南》翻译得更加具体。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f236442",
   "metadata": {},
   "source": [
    "#### 4.3.2 进阶深度版：长书籍英翻中"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8466ee7",
   "metadata": {},
   "source": [
    "导入书籍\n",
    "\n",
    "数据来源：https://github.com/LouisScorpio/datamining/tree/master/tensorflow-program/nlp/word2vec/dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e96eda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset/哈利波特1-7英文原版.txt\", \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbfcaf5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全书字符数:  6350735\n"
     ]
    }
   ],
   "source": [
    "print('全书字符数: ', len(text))\n",
    "# 整本书的字符数约有635万，但我们知道，chatgpt的api调用是根据token数量来的，\n",
    "# tokenizer本身的作用是将句子切分成单词，再将单词转化为数值型的输入，我们可以简单地使用tokenizer来统计token数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df425f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1673251 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全书token数:  1673251\n",
      "翻译全书约需16.73251美元\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")  # GPT-2的tokenizer和GPT-3是一样的\n",
    "token_counts = len(tokenizer.encode(text))\n",
    "print('全书token数: ', token_counts)\n",
    "\n",
    "# chatgpt的api调用价格是 1000 token 0.01美元，因此可以大致计算翻译一本书的价格\n",
    "translate_cost = 0.01 / 1000 * token_counts\n",
    "print(f'翻译全书约需{translate_cost}美元')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cbbe8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一册字符数:  442815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (119873 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一册token数:  119873\n",
      "翻译第一册约需1.19873美元\n"
     ]
    }
   ],
   "source": [
    "# 翻译全书约需115.14 rmb成本，有点贵了，我们试着只翻译第一本\n",
    "end_idx = text.find('2.Harry Potter and The Chamber Of Secrets.txt')\n",
    "text = text[:end_idx]\n",
    "print('第一册字符数: ', len(text))\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\") \n",
    "token_counts = len(tokenizer.encode(text))\n",
    "print('第一册token数: ', token_counts)\n",
    "\n",
    "translate_cost = 0.01 / 1000 * token_counts\n",
    "print(f'翻译第一册约需{translate_cost}美元')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c7f3be",
   "metadata": {},
   "source": [
    "GPT-3的token限制大约在4096左右（据说GPT-4最多输入3.2万token），因此无法直接将12万token的文本输进去。\n",
    "\n",
    "我们可以将使用一个简单的方法，将文本分成若干份，每一份使用chatgpt翻译，最终再拼接起来。\n",
    "\n",
    "首先，我们最好能保证每份文本本身的语义连贯性，如果从一个句子中间将上下文拆成两块，则翻译时容易存在歧义。\n",
    "\n",
    "一个比较直观的想法是，将每个段落当成一个文本块，每次翻译一段。\n",
    "\n",
    "但是本书的段落非常多，一段一段翻译显然会降低翻译的效率。同时，由于每段的上下文较少，导致翻译错误的可能性上升。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5103f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "段落数:  3038\n",
      "最长段落的token数:  275\n"
     ]
    }
   ],
   "source": [
    "paragraphs = text.split('\\n')\n",
    "print('段落数: ', len(paragraphs))\n",
    "\n",
    "ntokens = []\n",
    "for paragraph in paragraphs:\n",
    "    ntokens.append(len(tokenizer.encode(paragraph)))\n",
    "print('最长段落的token数: ', max(ntokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a7be46",
   "metadata": {},
   "source": [
    "因此，我们选定一个阈值，如500，每次加入一个文本段落，如果总数超过500，则开启一个文本块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce91a573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文本块数:  256\n",
      "最长文本块的token数:  500\n"
     ]
    }
   ],
   "source": [
    "def group_paragraphs(paragraphs, ntokens, max_len=1000):\n",
    "    \"\"\"\n",
    "    合并短段落为文本块，用于丰富上下文语境，提升文本连贯性，并提升运算效率。\n",
    "    :param paragraphs: 段落集合\n",
    "    :param ntokens: token数集合\n",
    "    :param max_len: 最大文本块token数\n",
    "    :return: 组合好的文本块\n",
    "    \"\"\"\n",
    "    batches = []\n",
    "    cur_batch = \"\"\n",
    "    cur_tokens = 0\n",
    "\n",
    "    # 对于每个文本段落做处理\n",
    "    for paragraph, ntoken in zip(paragraphs, ntokens):\n",
    "        if ntoken + cur_tokens + 1 > max_len:  # '1' 指的是'\\n'\n",
    "            # 如果加入这段文本，总token数超过阈值，则开启新的文本块\n",
    "            batches.append(cur_batch)\n",
    "            cur_batch = paragraph\n",
    "            cur_tokens = ntoken\n",
    "        else:\n",
    "            # 否则将段落插入文本块中\n",
    "            cur_batch += \"\\n\" + paragraph\n",
    "            cur_tokens += (1 + ntoken)\n",
    "    batches.append(cur_batch)  # 记录最后一个文本块\n",
    "    return batches\n",
    "\n",
    "batchs = group_paragraphs(paragraphs, ntokens, max_len=500)\n",
    "print('文本块数: ', len(batchs))\n",
    "\n",
    "new_tokens = []\n",
    "for batch in batchs:\n",
    "    new_tokens.append(len(tokenizer.encode(batch)))\n",
    "print('最长文本块的token数: ', max(new_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ece9d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.Harry Potter and the Sorcerer's Stone.txt\n",
      "\n",
      "　　Harry Potter and the Sorcerer's Stone\n",
      "　　CHAPTER ONE\n",
      "　　THE BOY WHO LIVED\n",
      "　　Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you'd expect to be involved in anything strange or mysterious, because they just didn't hold with such nonsense.\n",
      "　　Mr. Dursley was the director of a firm called Grunnings, which made drills. He was a big, beefy man with hardly any neck, although he did have a very large mustache. Mrs. Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbors. The Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere.\n",
      "　　The Dursleys had everything they wanted, but they also had a secret, and their greatest fear was that somebody would discover it. They didn't think they could bear it if anyone found out about the Potters. Mrs. Potter was Mrs. Dursley's sister, but they hadn't met for several years; in fact, Mrs. Dursley pretended she didn't have a sister, because her sister and her good-for-nothing husband were as unDursleyish as it was possible to be. The Dursleys shuddered to think what the neighbors would say if the Potters arrived in the street. The Dursleys knew that the Potters had a small son, too, but they had never even seen him. This boy was another good reason for keeping the Potters away; they didn't want Dudley mixing with a child like that.\n",
      "　　When Mr. and Mrs. Dursley woke up on the dull, gray Tuesday our story starts, there was nothing about the cloudy sky outside to suggest that strange and mysterious things would soon be happening all over the country. Mr. Dursley hummed as he picked out his most boring tie for work, and Mrs. Dursley gossiped away happily as she wrestled a screaming Dudley into his high chair.\n"
     ]
    }
   ],
   "source": [
    "# 展示第一段文本\n",
    "print(batchs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3adb5476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "欣欣夫妇对4号普里维特路的房子非常自豪，他们乐呵呵地表示，自己完全是一家正常家庭，也就不沾任何奇怪或神秘的事情。要是提到像这样的废话，他们实在是有点不屑一顾。欣欣先生当时正在格林宁公司任出品部主任，他长得又胖又壮，脖子很粗，只有一撮大胡子。欣欣太太瘦得非常，并且，出乎意料的是，她的脖子竟然站地比正常人都长，这样就非常省事，因为欣欣太太经常会在花园栅栏上俯瞰，窥探邻居的一举一动。欣欣夫妇有一个小儿子，叫达力，他们认为，没有比这孩子更好的了。\n",
      "\n",
      "欣欣夫妇有很多东西，但其实他们也拥有一个秘密，最怕的是被有人发现。他们实在不敢想象，如果波特家来他们街上，会有什么样的下场……波特太太就是欣欣太太的妹妹，但是他们已经好几年没有见过面了。实际上，欣欣太太压根就假装没有妹妹，因为妹妹和他那没用的丈夫，实在是与欣欣家的一切都格格不入。同时，欣欣夫妇还知道波特家有一个小儿子，但是他们从来都没有见过。这孩子可就是欣欣夫妇珍爱达力的另一个绝好理由，不要让达力和他有接触。\n",
      "\n",
      "当本故事发生的比较乏味的周二早晨，欣欣夫妇醒来，天空阴沉沉的，毫不暗示即将发生的奇异神秘的事情。欣欣先生嗯嗯哼哼地挑出今天上班穿的最没气质的领带，欣欣太太讨论着什么有趣的闲事，忙着把尖叫的达力抱进高脚高位椅里。\n"
     ]
    }
   ],
   "source": [
    "# 实操中发现，使用chatgpt翻译长文本很慢，这里改用Davinci实现，感兴趣的同学可以尝试优化\n",
    "# 速率限制见：https://platform.openai.com/docs/guides/rate-limits/overview\n",
    "# def translate_text(text):\n",
    "#     content = f\"请将以下英文文本翻译成中文:\\n{text}\"\n",
    "#     response = openai.ChatCompletion.create(\n",
    "#         model=\"gpt-3.5-turbo\", \n",
    "#         messages=[{\"role\": \"user\", \"content\": content}]\n",
    "#     )\n",
    "#     translated_text = response.get(\"choices\")[0].get(\"message\").get(\"content\")\n",
    "#     return translated_text\n",
    "def translate_text(text):\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=f\"请将以下英文翻译成中文:\\n{text}\",\n",
    "        max_tokens=2048\n",
    "    )\n",
    "\n",
    "    translate_text = response.choices[0].text.strip()\n",
    "    return translate_text\n",
    "print(translate_text(batchs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c3c95a",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来，我们对每个文本块做翻译，并将结果合并起来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "373e2648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "translated_batchs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "67f95395",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 8/8 [07:50<00:00, 58.79s/it]\n"
     ]
    }
   ],
   "source": [
    "# 有的时候由于VPN等问题，可能会出现断联，也即443 timeout，可以在断点batch处重连\n",
    "translated_batchs_bak = translated_batchs.copy()\n",
    "cur_len = len(translated_batchs)\n",
    "for i in tqdm(range(cur_len, len(batchs))):\n",
    "    translated_batchs.append(translate_text(batchs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "81bbc279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存结果至txt文件\n",
    "result = '\\n'.join(translated_batchs)\n",
    "\n",
    "with open('dataset/哈利波特1中文版翻译.txt','w', encoding='utf-8') as f:\n",
    "    f.write(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7674c808",
   "metadata": {},
   "source": [
    "## 相关文献"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10330b6",
   "metadata": {},
   "source": [
    "- 【1】[自然语言生成介绍](https://easyai.tech/ai-definition/nlg/)\n",
    "- 【2】[fine tune tutorial](https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset)\n",
    "- 【3】[文本摘要任务介绍](https://zhuanlan.zhihu.com/p/83596443)\n",
    "- 【4】[中文文本纠错算法整体介绍](https://zhuanlan.zhihu.com/p/583590525)\n",
    "- 【5】[中文文本纠错任务简介](https://zhuanlan.zhihu.com/p/545776580)\n",
    "- 【6】[pycorrector介绍](https://github.com/shibing624/pycorrector)\n",
    "- 【7】[长文本英翻中tutorial](https://github.com/openai/openai-cookbook/blob/main/examples/book_translation/translate_latex_book.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
